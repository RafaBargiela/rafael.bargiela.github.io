# Title: Assesment of Metagenome Assembled Genomes (MAGs)
## Last update: 4th of June, 2025
## PI: Prof. Manuel Ferrer, Dr. Rafael Bargiela

On this tutorial we will describe the pipeline to get and analyse Metagenome Assembled Genomes (MAGs) from environmental metagenomic samples. Before to proceed with the pipeline itself to get the MAGs, we will need to assemble the metagenomes first, in order to get one __assembly per sample__. Additionally, when we have more than one sample per sample type, we can performed a __co-Assembly__, joinning processing reads from samples of the same type together, getting an assembly with higher depth an coverage per base. Take into account that for this tutorial we will assume that reads have been generated by Pair-end sequencing.
## Dependencies

- BBtools 
- MEGAHIT
- MetaBAT2
- MaxBin2
- Concoct
- DAS_Tool
- Magpurify
- CheckM2
- GDTB-tk

## 1. Assemblies per samples and Co-Assemblies
On one side, we will proceed to assemble each sample independently and get MAGs from each sample individually. However, when we have more than one replicate for one sample, or samples of the same type and treatment, we can make a common assembly in order to improve coverage. In this case, we will have several forward and reverse files corresponding to each indenpendent type/treatment, so it would be good to create a co-assembly. First, we will preprocess the raw reads with BBtools for the removal of artifacts and adapters, re-pair unpaired reads, read correction. In both caes, individual assemblies and co-assemblies, preprocessing of reads is the same:

From now on, $DIR value represents the directory where the whole environment, treatment or group of samples are analyzed. In some cases, $DIR is also used to provided prefix to downstream output file names. $PREOUT is the directory where the pre-treatment of each individual sample is run and $NAME will correspond to each individual sample name.
Re-pairing reads:
```bash
            repair.sh in="${R1}" in2="${R2}" out="${DIR}/${PREOUT}/${NAME}.R1.repaired.fastq.gz" out2="${DIR}/${PREOUT}/${NAME}.R2.repaired.fastq.gz" repair=t outs="${DIR}/${PREOUT}/${NAME}.singletons.fastq.gz" 
```
Removing artifacts and adapters:
```bash
            bbduk.sh in="${DIR}/${PREOUT}/${NAME}.R1.repaired.fastq.gz" in2="${DIR}/${PREOUT}/${NAME}.R2.repaired.fastq.gz" out="${DIR}/${PREOUT}/${NAME}.R1.artifacts.fastq.gz" outm="${DIR}/${PREOUT}/${NAME}.R1.artifacts.discard.fastq.gz" out2="${DIR}/${PREOUT}/${NAME}.R2.artifacts.fastq.gz" outm2="${DIR}/${PREOUT}/${NAME}.R2.artifacts.discard.fastq.gz" hdist=1 k=31 ftm=5 ref=artifacts threads="${CORES}"
            bbduk.sh in="${DIR}/${PREOUT}/${NAME}.R1.artifacts.fastq.gz" in2="${DIR}/${PREOUT}/${NAME}.R2.artifacts.fastq.gz" out="${DIR}/${PREOUT}/${NAME}.R1.clean.fastq.gz" out2="${DIR}/${PREOUT}/${NAME}.R2.clean.fastq.gz"  ktrim=r k=23 mink=11 hdist=1 tbo=t qtrim=r trimq=20 ref=adapters threads="${CORES}"
```
Correcting reads:
```bash
             tadpole.sh in="${DIR}/${PREOUT}/${NAME}.R1.clean.fastq.gz" in2="${DIR}/${PREOUT}/${NAME}.R2.clean.fastq.gz" out="${DIR}/${PREOUT}/${NAME}.R1.corrected.fastq.gz" out2="${DIR}/${PREOUT}/${NAME}.R2.corrected.fastq.gz" mode=correct threads="${CORES}"
```
Merging reads from same sample:
```bash
              bbmerge.sh in="${DIR}/${PREOUT}/${NAME}.R1.clean.fastq.gz" in2="${DIR}/${PREOUT}/${NAME}.R2.clean.fastq.gz" out="${DIR}/${PREOUT}/${NAME}.Merged.fastq.gz" outu1="${DIR}/${PREOUT}/${NAME}.R1.unmerged.fastq.gz" outu2="${DIR}/${PREOUT}/${NAME}.R2.unmerged.fastq.gz" ihist="${DIR}/${PREOUT}/${NAME}.insert_size.txt" usejni=t 
```
Steps above are common for both individual asemblies and co-assemblies. For co-assemblies, independent files of each type/treatment generated at the end of the merging step, need to be concatenated, joining each independent sample file into a common one per type. So, finally we will have always three files:
```bash
  cat $DIR/$PREOUT/*R1.unmerged.fastq.gz > $DIR/$OUT/R1.unmerged.concat.fastq.gz
  cat $DIR/$PREOUT/*R2.unmerged.fastq.gz > $DIR/$OUT/R2.unmerged.concat.fastq.gz
  cat $DIR/$PREOUT/*.Merged.fastq.gz > $DIR/$OUT/Merged.concat.fastq.gz
```
Final files for the co-assembly will be stored in a diferent folder called $OUT.
So, to start the co-assembly we have three final files: all concatenated merged reads, all R1 reads unmerged concatenated, and all R2 unmerged concatenated. To perform the co-assembly we will use MEGAHIT with the following command:
```bash
  megahit --read "${MERGED}" -1 "${R1}" -2 "${R2}" -o "${DIR}/${OUT}/MEGAHIT" --out-prefix "${DIR}" -t "${CORES}" --presets meta-sensitive --min-contig-len 1000
```
We will limit the size of contigs to 1,000.
**NOTE:** It would be of good practice to ensure all co-assembly contigs in the fasta file have a simple header:
```
>k141_12955425
```
instead of something longer as usually megahit returns the fasta file:
```
>k141_12955425 flag= ...
```
In that case, edit the file to simply the headers. To do that, we can use _rename.sh_:
```bash
  rename.sh in="${DIR}/${OUT}/MEGAHIT/${DIR}.contigs.fa" out="MEGAHIT_CO-ASSEMBLIES/${DIR}.megahit.contigs.fasta" prefix=Contig
```
So, after all this process, we will have individual sample assemblies and co-assemblies for each type or treatment.



## 2. Mapping samples reads to co-assembly
Reads from each individual sample must be mapped to the resulting (co-)assembly, in order to get coverage, depth and abundance data for each contig. This is necessary for the next binning step with the different tools we will use. Hereafter, all mapping files will be stored in the folder called "BAM".

### 2.1. Indexing co-assembly and mapping individual samples
Firt, we need to create and index of the (co-)assembly using bowtie2 and storing all files produced at BAM directory:
```bash
  bowtie2-build -p --threads "${CORES}" "MEGAHIT_CO-ASSEMBLIES/${DIR}.megahit.contigs.fasta" "${DIR}/BAM/${DIR}.index"
```
Now, need to map each sample reads over the (co-)assembly index:
```bash
  bowtie2 -x "${DIR}/BAM/${DIR}.index" -q -1 "$R1" -2 "$R2" --no-unal -p 4 -S "${DIR}/BAM/${NAME}".sam --threads "${CORES}"
  samtools view -b -o "${DIR}/BAM/${NAME}".raw.bam "${DIR}/BAM/${NAME}".sam
  samtools sort -o "${DIR}/BAM/${NAME}".bam "${DIR}/BAM/${NAME}".raw.bam
  samtools index "${DIR}/BAM/${NAME}".bam
```
Besides the values used already on steps above, $R1 and $R2 correspond here to the forward and reverse reads of each of the individual samples. _samtools_ is used to create sam files the final sorted bam files. 

### 2.2. Getting coverage and abundance files
Sorted .bam files created in the step above will be used as input by the binning software. However, not all binning software works the same way, some use directly the .bam files, others provide specific scripts to create their specific input files using the .bam files, and others need you to provide specific coverage or abundance files in a specific format. We will work with three different binning programs: Metabat2, concoct and MaxBin2. Note that in the following explanation the "MAGs" folder is where binning programs will be run afterwards.

In case of __Metabat2__, it provides the specific script jgi_summarize_bam_contig_depths, included in its installation, to create _depth_ input files from the .bam files, which you can add all together with special characters:
```bash
    jgi_summarize_bam_contig_depths --outputDepth "${DIR}"/MAGs/metabat2_depth.txt "${DIR}"/BAM/*.bam
```

__Concoct__ needs a coverage file as input to run. Two previous steps before running the command are required. First, it needs to cut the assembly file in chunks, 10k will be enough, like this:
```bash
    cut_up_fasta.py "${ASS}" -c 10000 -o 0 --merge_last -b "${DIR}"/MAGs/"${DIR}"_10K.bed > "${DIR}"/MAGs/"${DIR}"_10K.fa
```
then, the coverage table will be constructed using .bed file created above with the next script:
```bash
    concoct_coverage_table.py "${DIR}"/MAGs/"${DIR}"_10K.bed "${DIR}"/BAM/*.bam > "${DIR}"/MAGs/concoct_coverage_table.tsv
```
When running _concoct_, it will use both, chunks on _"${DIR}"_10k.fa_ file and the coverage table on _concoct_coverage_table.tsv_.

Finally, __MaxBin2__ needs to create abundance files, with then will be stored in a list file used as input. To do this, first we will use _pileup.sh_ from bbtools. Rather than the .bam files, it uses the __.sam__ files created in the first mapping step.
```bash
      pileup.sh in="${DIR}/BAM/${NAME}".sam out="${DIR}/BAM/${NAME}".cov.txt 
```
This will be done for each of the samples, so we will have a cov.txt file for each sample mapped over the assembly. From each of them, we need to patch the contig name and the abundance from the cov.txt table to create the specific abundance file for MaxBin2:
```bash
    while IFS=$'\t' read -r -a line
    do
        if [[ "${line[0]}" =~ ^# ]]
        then
            continue
        else
          echo -e "${line[0]}\t${line[4]}" >> "${DIR}/BAM/${NAME}.ab.txt"
        fi
    done < "${DIR}/BAM/${NAME}.cov.txt" 
```
There are many ways to do this. You can use a custom Perl or Python script, or a command line for awk, not necessarily this one. Finally, we store the path to all the ab.txt in a list file:
```bash
    echo "${PWD}/${DIR}/BAM/${NAME}.ab.txt" >> "${DIR}/MAGs/MaxBin2.Ab.List.txt"
```
This Ab.List.txt is what we will use below as input to MaxBin2.

## 3. Binning process
Using the metagenomic assembly and the files created in the step above, we will proceed to get the raw bins for each of the samples using the three mentioned applications: _MetaBAT2_, _MaxBin2_ and _Concoct_. We will do this using the following commands:

```bash
# Running Metabat2
metabat2 -i "${ASSEMBLY}" -a "${DIR}/${NAME}.metabat2_depth.txt" -o "${DIR}/${BINOUT}/${NAME}".metabat2.bin -t "${CORES}" -m $MBAT2MINCONTIG --maxP 90 &> "${DIR}/${NAME}".metabat2.STDouterr.txt

# Running Concoct
concoct --composition_file "${DIR}/${NAME}"_10K.fa --coverage_file "${DIR}/${NAME}concoct_coverage_table.tsv" -b "${DIR}"/concoct_output/ --threads "${CORES}" &> "${DIR}/${NAME}.concoct.STDouterr.txt"
merge_cutup_clustering.py "${DIR}"/concoct_output/clustering_gt1000.csv > "${DIR}"/concoct_output/clustering_merged.csv
extract_fasta_bins.py "${ASSEMBLY}" "${DIR}"/concoct_output/clustering_merged.csv --output_path "${DIR}"/concoct_output/bins

# Running MaxBin2
perl run_MaxBin.pl -contig "${ASSEMBLY}" -abund_list "${DIR}/BAM/${NAME}.MaxBin2.Ab.List.txt" -min_contig_length "${MBIN2MINCONTIG}" -thread "${CORES}" -out "${DIR}/${NAME}.maxbin2" &> "${DIR}/${NAME}MaxBin2.STDouterr.txt"
```

Once we have run the three programs, we will obtain a set of different number of bins, which eventually will be redundant among the three tools used. Therefore, we have to dereplicate the this dataset in order to have a final set of non-redundant MAGs.

## 4. Dereplication of initial bins
For dereplication, we will use the tool _DAS_tool_, which will produce a non-redundant set of MAGs. We need all the bins in the same folder and three different files for each binning program (_MetaBAT2_, _MaxBin2_ and _Concoct_) which will be a two column table with the names of contigs of each MAG in the first column and the MAG ID in the other one. These will be three different tables, one for each binning tool, looking like this:

```
Contig_538      DONOR.maxbin2.001
Contig_23701    DONOR.maxbin2.001
Contig_47727    DONOR.maxbin2.001
Contig_734      DONOR.maxbin2.002
Contig_1721     DONOR.maxbin2.002
Contig_9517     DONOR.maxbin2.002
Contig_9525     DONOR.maxbin2.002
Contig_12700    DONOR.maxbin2.002
```
 We will name these files "{SAMPLENAME}.{Concoct|Metabat2|MaxBin2}.Contigs2Bins.tsv". For its use we will use the following commands:

```bash
DAS_Tool -i "${C2B}" -l "${LAB}" -c "${ASSEMBLY}" -o "${DIR}/UNREFint/${NAME}.dastool" --score_threshold 0 --write_bins --threads "${CORES}"
```
Where ${C2B} is a comma-separated list of the three Contig2Bins.tsv files and ${LAB} is a comma-separated list of the binners used ("metabat2,concoct,maxbin2"). Note that _DAS_Tool_ only detects the fasta files in with the extension **.fasta**, so you need to previously rename the raw bins in order to dereplicate. Once dereplication is over, it is recommended to rename again the resulting dereplicated MAGs to **.fna** extension.

## 5. Decontamination and purification of MAGs
Resulting dereplicated MAGs have now to be purified. To do this we will use the too _MagPurify2_. This tool use different pipelines and filters to purify each MAG, based on taxonomy, codon usage, tetranucleotide frequencies, contig converage and contaminant detection. For the taxonomy filter, you will need to download their database for the website and add it to the command line (hereafter $MAGPURIFY2DB). Also, we will use the coverage file created previously using _bbtools_ with _pileup.sh_. You can run directly all the filters by the following command:

```bash
magpurify2 end_to_end --threads "${CORES}" --taxonomy_database $MAGPURIFY2DB --coverage_file "${DIR}/BAM/${NAME}.cov.txt" $DIR/$FOLDER2REF/* "${DIR}/REF_MAGs/" "${DIR}/${REFOUT}/" &> "${DIR}/${NAME}.Magpurify2.stderr.txt"
```
Otherwise, you can run the different filters as follows:
```bash
echo "-- Running composition module"
magpurify2 composition --threads "${CORES}" $DIR/$FOLDER2REF/* "${DIR}/REF_MAGs/" >> "${DIR}/${NAME}.Magpurify2.stderr.txt" 2>&1
echo "-- Running covegage module"
magpurify2 coverage --threads "${CORES}" --coverage_file "${DIR}/BAM/${NAME}.cov.txt" $DIR/$FOLDER2REF/* "${DIR}/REF_MAGs/" >> "${DIR}/${NAME}.Magpurify2.stderr.txt" 2>&1
echo "-- Running codon_usage module"
magpurify2 codon_usage --threads "${CORES}" $DIR/$FOLDER2REF/* "${DIR}/REF_MAGs/" >> "${DIR}/${NAME}.Magpurify2.stderr.txt" 2>&1
echo "-- Running taxonomy module"               
magpurify2 taxonomy --threads "${CORES}" $DIR/$FOLDER2REF/* "${DIR}/REF_MAGs/" $MAGPURIFY2DB >> "${DIR}/${NAME}.Magpurify2.stderr.txt" 2>&1

```
where $FOLDER2REF is the folder where dereplicated where stored. Finally, will filter out the refined MAGs with the command _filter_:
```bash
magpurify2 filter --threads "${CORES}" $DIR/$FOLDER2REF/* "${DIR}/REF_MAGs/" "${DIR}/${REFOUT}/" >> "${DIR}/${NAME}.Magpurify2.stderr.txt" 2>&1
```

## 6. Quality Control
To assess the quality of the MAGs, with their completeness and level of contamiantion, alongside other parameters, wi will use _Checkm2_. To use this tool, we use the following command:

```bash
checkm2 predict -i "${DIR}/${REFOUT}" -o "${DIR}/${QCOUT}/DEREP_REFINED" -t "${CORES}" -x ".fna" --force &> "${DIR}/${NAME}.CheckM2.stderr.txt"

```

## 6. Taxonomic Classification of MAGs
the final step is to get the taxonomic classification of each MAGs. _Checkm2_ already perform an approach to this, but get a more specific an accurate assignation we will based on the Genome Taxonomy Database (https://gtdb.ecogenomic.org/) and their tool _GTDB-tk_ (https://github.com/Ecogenomics/GTDBTk). To use it, we need to download also their database (which is huge, to download here: https://ecogenomics.github.io/GTDBTk/installing/index.html#). For the last version of the database, we will need to have the last version of the software, so make sure of this. This software will be used with the following command with the pipeline classify_wf:
```bash
gtdbtk classify_wf --genome_dir "${GENOMEDIR}" --out_dir "${DIR}/${CLASSOUT}/DEREP_REFINED" --skip_ani_screen --cpus "${CORES}" --extension "fna" --force &> "${DIR}/${NAME}.GTDBtk.stderr.txt"

```
Here ${GENOMEDIR} is the folder where the final MAGs have been stored. For memory reasons, we will skip ANI screening.




